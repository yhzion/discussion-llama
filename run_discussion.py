#!/usr/bin/env python
"""
í† ë¡  ì‹œë®¬ë ˆì´ì…˜ì„ ì‹¤í–‰í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
"""

import os
import sys
import argparse
from discussion_llama.role.role_manager import RoleManager
from discussion_llama.engine.discussion_engine import DiscussionEngine
from discussion_llama.llm.llm_client import create_llm_client
from discussion_llama.engine.consensus_detector import ConsensusDetector

def main():
    # ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±
    parser = argparse.ArgumentParser(description="Run a discussion simulation")
    parser.add_argument("topic", help="The topic for discussion")
    parser.add_argument("--roles", help="Comma-separated list of role names to include")
    parser.add_argument("--num-roles", type=int, default=3, help="Number of roles to include (if not specified)")
    parser.add_argument("--max-turns", type=int, default=10, help="Maximum number of turns")
    parser.add_argument("--llm-client", default="mock", choices=["mock", "ollama", "enhanced_ollama"], 
                        help="LLM client to use")
    parser.add_argument("--model", default="llama2:7b-chat-q4_0", help="Model to use (for Ollama)")
    parser.add_argument("--max-retries", type=int, default=3, help="Maximum number of retries for LLM requests")
    parser.add_argument("--retry-delay", type=float, default=1.0, help="Delay between retries for LLM requests")
    parser.add_argument("--timeout", type=int, default=30, help="Timeout for LLM requests in seconds")
    parser.add_argument("--output", help="Output file for discussion results (JSON)")
    parser.add_argument("--stream", action="store_true", help="Use streaming for response generation (enhanced_ollama only)")
    parser.add_argument("--language", choices=["auto", "en", "ko"], default="auto", 
                        help="Language for the discussion (auto, en, ko). Default is auto-detect.")
    args = parser.parse_args()
    
    # ì—­í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
    roles_dir = "./roles"
    
    # ì—­í•  ê´€ë¦¬ì ì´ˆê¸°í™”
    try:
        role_manager = RoleManager(roles_dir)
    except Exception as e:
        print(f"Error initializing RoleManager: {e}")
        sys.exit(1)
    
    # ì—­í•  ì„ íƒ
    if args.roles:
        # ì§€ì •ëœ ì—­í•  ì‚¬ìš©
        role_names = args.roles.split(",")
        selected_roles = []
        for role_name in role_names:
            role = role_manager.get_role(role_name)
            if role:
                selected_roles.append(role)
            else:
                print(f"Warning: Role '{role_name}' not found")
    else:
        # ì£¼ì œì— ê¸°ë°˜í•œ ì—­í•  ìë™ ì„ íƒ
        selected_roles = role_manager.select_roles_for_discussion(args.topic, args.num_roles)
    
    if not selected_roles:
        print("Error: No valid roles selected for discussion")
        sys.exit(1)
    
    # LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    llm_kwargs = {"model": args.model}
    
    # Add enhanced_ollama specific parameters
    if args.llm_client == "enhanced_ollama":
        llm_kwargs.update({
            "max_retries": args.max_retries,
            "retry_delay": args.retry_delay,
            "timeout": args.timeout
        })
    
    llm_client = create_llm_client(args.llm_client, **llm_kwargs)
    
    # ì–¸ì–´ ê°ì§€
    if args.language == "auto":
        detected_language = llm_client.detect_language(args.topic) if hasattr(llm_client, 'detect_language') else 'en'
    else:
        detected_language = args.language
    
    # ì–¸ì–´ì— ë”°ë¥¸ ë©”ì‹œì§€ ì¶œë ¥
    if detected_language == 'ko':
        print(f"\nğŸš€ ì£¼ì œì— ëŒ€í•œ í† ë¡ ì„ ì‹œì‘í•©ë‹ˆë‹¤: {args.topic}")
        print(f"ğŸ‘¥ ì°¸ê°€ì: {', '.join(role.role for role in selected_roles)}")
        print("-" * 80)
    else:
        print(f"\nğŸš€ Starting discussion on topic: {args.topic}")
        print(f"ğŸ‘¥ Participants: {', '.join(role.role for role in selected_roles)}")
        print("-" * 80)
    
    # í•©ì˜ ê°ì§€ê¸° ìƒì„±
    consensus_detector = ConsensusDetector(llm_client)
    
    # í† ë¡  ì—”ì§„ ìƒì„± ë° ì‹¤í–‰
    state_dir = "./discussion_state"
    os.makedirs(state_dir, exist_ok=True)
    
    # LLM í´ë¼ì´ì–¸íŠ¸ë¥¼ DiscussionEngineì— ì „ë‹¬
    engine = DiscussionEngine(args.topic, selected_roles, state_dir, llm_client=llm_client)
    engine.max_turns = args.max_turns
    
    # ìŠ¤íŠ¸ë¦¬ë° ì˜µì…˜ ì„¤ì •
    engine.use_streaming = args.stream and args.llm_client == "enhanced_ollama"
    
    # í† ë¡  ì‹¤í–‰
    result = engine.run_discussion()
    
    # ê²°ê³¼ í‘œì‹œ
    if detected_language == 'ko':
        print("\nğŸ“ í† ë¡  ìš”ì•½:")
        print("-" * 80)
        
        for message in result["discussion"]:
            print(f"[{message['role']}]: {message['content']}")
            print("-" * 80)
        
        print(f"\nğŸ í† ë¡ ì´ {result['turns']}í„´ í›„ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        if result["consensus_reached"]:
            print("âœ… í•©ì˜ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤!")
        else:
            print("âŒ í•©ì˜ì— ë„ë‹¬í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        
        # ê²°ê³¼ ì €ì¥
        if args.output:
            import json
            with open(args.output, "w", encoding="utf-8") as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ ê²°ê³¼ê°€ {args.output}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nğŸ“ Discussion Summary:")
        print("-" * 80)
        
        for message in result["discussion"]:
            print(f"[{message['role']}]: {message['content']}")
            print("-" * 80)
        
        print(f"\nğŸ Discussion ended after {result['turns']} turns.")
        if result["consensus_reached"]:
            print("âœ… Consensus was reached!")
        else:
            print("âŒ No consensus was reached.")
        
        # ê²°ê³¼ ì €ì¥
        if args.output:
            import json
            with open(args.output, "w", encoding="utf-8") as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ Results saved to {args.output}")

if __name__ == "__main__":
    main() 